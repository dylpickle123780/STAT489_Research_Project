---
title: "Random_Forest_Project"
format: pdf
editor: visual
---

## RF on Total Data

Below is the set up for the Random Forest where we used centroid geometry to establish the dataset. We then split the data by the price adjusted delta which is an old metric that we used that is the difference from the mean difference of the FMR and Apartment data. Then, created a partition between the training and testing data for an 4:1 split. Finally for the set up, we created the compute cluster (10 logical processors in my case) to start the training block.

```{r}
#Load
library(tidyverse)
library(foreach)
library(doParallel)
library(doRNG)
library(sf)
library(tigris)
library(caret)

load("../Data/Total_data.Rdata")
set.seed("123780")

#Turn multipolygon into centroid of each county for purpose of passing
#latitude and longitude into the random forest model to account for spatial effects
random_f_data = total_data %>% 
  mutate(geometry = st_centroid(geometry)) %>% 
  mutate(longitude = st_coordinates(geometry)[,1], 
         latitude = st_coordinates(geometry)[,2]) %>% 
  select(-price:-price_delt) %>% 
  st_drop_geometry()


# Create data split
## First, create training/testing split

rf_split = createDataPartition(random_f_data$price_delt_adj,
                                       p = 0.8,
                                       )[[1]]

apartment_train = random_f_data[rf_split,]
apartment_test = random_f_data[-rf_split,]

# Set up parallel training
compute_cluster = makeCluster(10)
registerDoParallel(compute_cluster)
```

We then created the Random Forest Grid that we train the data on with the function calls that the RF_implementation has which is functions like training and control functions as we have learned in class. It also contains a prediction function so we can layer it over the whole United States.

```{r}

####### Random Forest #########
source("../Functions/RF_implementation.R")

#Use number of parameters
rfGrid = expand.grid(
  mtry = c(1, 2, 3, 4, 5, 6)
)

random_f_model = run_rf_model(apartment_train, rfGrid)


# Random Forest predictions
model_predictions = rf_prediction(random_f_model, apartment_test)


root_mean = RMSE(model_predictions, apartment_test$price_delt_adj)
mean_absolute = MAE(model_predictions, apartment_test$price_delt_adj)
mean_squared = mean((model_predictions - apartment_test$price_delt_adj)^2)

comparison_metrics = c(root_mean, mean_absolute, mean_squared)

```

```{r}
#run rf with full dataset
full_random_f_model = run_rf_model(random_f_data, rfGrid)

full_model_predictions = rf_prediction(full_random_f_model, random_f_data)

#add predictions to dataset
total_data$predictions = full_model_predictions

stopCluster(compute_cluster)

```

The plotting data is incomplete because the foundational data does not contain all the counties however it provides a prediction value for all the datapoints.

```{r}
#plotting fitted values
plotting_rf_predicted(total_data)
```

```{r}
library(tigris)
county_sh = states()
county_sh = county_sh %>% 
  filter(STATEFP!="15",STATEFP!="02",STATEFP!="78",STATEFP!="72",
         STATEFP!="69",STATEFP!="60",STATEFP!="66") %>% 
  select(geometry)

#make polygons 
US_covering_polys = st_make_grid(county_sh, n = c(80, 80)) %>% 
  st_intersection(county_sh) %>% 
  st_sf()

#fill with median
filled_random_f_data = US_covering_polys %>% 
  mutate(geometry = st_centroid(geometry)) %>% 
  mutate(longitude = st_coordinates(geometry)[,1], 
         latitude = st_coordinates(geometry)[,2]) %>% 
  st_drop_geometry() %>% 
  mutate(
    Count = median(total_data$Count),
    bedrooms = median(total_data$bedrooms),
    bathrooms = median(total_data$bathrooms),
    square_feet = median(total_data$square_feet)
  )

filled_predictions = rf_prediction(full_random_f_model, filled_random_f_data)

US_covering_polys = US_covering_polys %>% 
  mutate(predictions = filled_predictions)

ggplot(US_covering_polys) +
  geom_sf(aes(fill = predictions))+
  scale_fill_gradientn(colours = terrain.colors(8))
```
